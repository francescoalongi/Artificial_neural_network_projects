{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model notebook\n",
    "\n",
    "In this notebook we are going to define and train the CNN model used for the classification task.\n",
    "\n",
    "## Setting up the stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "#Import the custom script\n",
    "import ValidationCreation\n",
    "import DatasplitJsonCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "datasetDir = os.path.join(cwd,'Classification_Dataset')\n",
    "\n",
    "#Classes for our problem\n",
    "\n",
    "classes = ['owl',\n",
    "'galaxy',\n",
    "'lightning',\n",
    "'wine-bottle',\n",
    "'t-shirt',\n",
    "'waterfall',\n",
    "'sword',\n",
    "'school-bus',\n",
    "'calculator',\n",
    "'sheet-music',\n",
    "'airplanes',\n",
    "'lightbulb',\n",
    "'skyscraper',\n",
    "'mountain-bike',\n",
    "'fireworks',\n",
    "'computer-monitor',\n",
    "'bear',\n",
    "'grand-piano',\n",
    "'kangaroo',\n",
    "'laptop']\n",
    "\n",
    "seed = 1234\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions in the external scripts we created in order to split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidationCreation.create_validation(ratio = 0.2)\n",
    "DatasplitJsonCreate.DatasetSplitJSON()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the various directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDir = os.path.join(datasetDir,'training')\n",
    "tb_dir = os.path.join(cwd,'tensorboard')\n",
    "sv_dir = os.path.join(cwd,'saved_model')\n",
    "csv_dir = os.path.join(cwd,'csv_savings')\n",
    "vd_dir = os.path.join(datasetDir,'validation')\n",
    "\n",
    "if not os.path.exists(tb_dir):\n",
    "    os.makedirs(tb_dir)\n",
    "else:\n",
    "    for(root,dirs,files) in os.walk(tb_dir,topdown=False):\n",
    "        for name in files:\n",
    "            os.remove(os.path.join(root, name))\n",
    "        for name in dirs:\n",
    "            os.rmdir(os.path.join(root,name))\n",
    "\n",
    "if not os.path.exists(sv_dir):\n",
    "    os.makedirs(sv_dir)\n",
    "\n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)\n",
    "\n",
    "now = datetime.now().strftime('%b%d_%H-%M-%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the generator for both training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "augment_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                                                rotation_range=20,\n",
    "                                                width_shift_range=20,\n",
    "                                                height_shift_range=20,\n",
    "                                                zoom_range=0.2,\n",
    "                                                horizontal_flip=True,\n",
    "                                                fill_mode='constant',\n",
    "                                                cval = 0,\n",
    "                                                shear_range = 0.2,\n",
    "                                                rescale=1./255\n",
    "                                                )\n",
    "\n",
    "bs = 20 #training Batch Size\n",
    "bsv = 10 #validation Batch Size\n",
    "\n",
    "h = 200 #height input\n",
    "w= 200 #width input\n",
    "\n",
    "valid_generator = dg.flow_from_directory(directory=vd_dir,\n",
    "                                         batch_size = bsv,\n",
    "                                         classes = classes,\n",
    "                                         target_size= (h,w),\n",
    "                                         class_mode = 'categorical',\n",
    "                                         color_mode = 'rgb',\n",
    "                                         seed = seed,\n",
    "                                         shuffle = False)\n",
    "\n",
    "train_gen = augment_generator.flow_from_directory(directory=trainingDir,\n",
    "                                                   target_size =(h,w),\n",
    "                                                   batch_size=bs,\n",
    "                                                   classes = classes,\n",
    "                                                   class_mode = 'categorical',\n",
    "                                                   shuffle=True,\n",
    "                                                   seed = seed,\n",
    "                                                   color_mode = 'rgb'\n",
    "                                                   )\n",
    "\n",
    "# Creation of the datasets for training and validation\n",
    "\n",
    "trainingDataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
    "                                                 output_types = (tf.float32,tf.float32),\n",
    "                                                 output_shapes = ([None,h,w,3],[None,20]))\n",
    "\n",
    "validationDataset = tf.data.Dataset.from_generator(lambda: valid_generator,\n",
    "                                                  output_types = (tf.float32,tf.float32),\n",
    "                                                  output_shapes =([None,h,w,3],[None,20]))\n",
    "\n",
    "trainingDataset.repeat()\n",
    "validationDataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for the creation of the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(results, results_dir='./'):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(key + ',' + str(value) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the CNN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss, Metrics, Optimizer and others\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "metrics = ['accuracy']\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001,momentum=0.7)\n",
    "regularizer = tf.keras.regularizers.l2(0.01)\n",
    "\n",
    "# Feature Extractor\n",
    "\n",
    "fe = tf.keras.Sequential()\n",
    "fe.add(tf.keras.layers.Conv2D(filters=64,input_shape=(h,w,3),kernel_size=(3,3),padding='same',activation=tf.keras.activations.relu))\n",
    "fe.add(tf.keras.layers.MaxPool2D(pool_size=(4,4)))\n",
    "fe.add(tf.keras.layers.Conv2D(filters=128,kernel_size=(3,3),padding='same',activation=tf.keras.activations.relu))\n",
    "fe.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "fe.add(tf.keras.layers.Conv2D(filters=128,kernel_size=(3,3),padding='same',activation=tf.keras.activations.relu))\n",
    "fe.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "fe.add(tf.keras.layers.Conv2D(filters=256,kernel_size=(3,3),padding='same',activation=tf.keras.activations.relu))\n",
    "fe.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#Fully Connected\n",
    "\n",
    "fc = tf.keras.Sequential()\n",
    "fc.add(tf.keras.layers.Flatten())\n",
    "fc.add(tf.keras.layers.Dense(700,activation=tf.keras.activations.relu))\n",
    "fc.add(tf.keras.layers.Dropout(0.3))\n",
    "fc.add(tf.keras.layers.Dense(600,activation=tf.keras.activations.relu))\n",
    "fc.add(tf.keras.layers.Dropout(0.3))\n",
    "fc.add(tf.keras.layers.Dense(20,activation = tf.keras.activations.softmax))\n",
    "\n",
    "#Complete Model\n",
    "\n",
    "model = tf.keras.Sequential([fe,fc])\n",
    "model.compile(optimizer= optimizer, loss = loss, metrics = metrics)\n",
    "model.summary()\n",
    "fe.summary()\n",
    "fc.summary()\n",
    "\n",
    "#Function for the Learning rate scheduler used in the fit method\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 20:\n",
    "        return 0.04\n",
    "    elif epoch >= 20 and epoch <= 30:\n",
    "        return 0.01\n",
    "    else:\n",
    "        return 0.01 * 1.0/(1.0+ (epoch*0.07))\n",
    "\n",
    "#Callbacks\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir = tb_dir,histogram_freq=1)\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler,1)\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',patience=3)\n",
    "\n",
    "#Fitting for the model\n",
    "\n",
    "model.fit(x = trainingDataset,\n",
    "         epochs=50, verbose=2,\n",
    "         steps_per_epoch = len(train_gen),\n",
    "         validation_data = validationDataset,\n",
    "         validation_steps = len(valid_generator),\n",
    "         callbacks = [tb_callback,lr_scheduler, es_callback]\n",
    "         )\n",
    "\n",
    "#Save the structure of the model in a json file\n",
    "\n",
    "m = model.to_json(indent=3)\n",
    "with open(os.path.join(sv_dir,'json_model_'+now+'.json'), 'w') as out:\n",
    "    out.write(m)\n",
    "\n",
    "#Save the model in the h5 format\n",
    "\n",
    "model.save(os.path.join(sv_dir,'CNN_model'+now+'.h5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the results on the model\n",
    "\n",
    "results = {}\n",
    "print('Test set predictions...')\n",
    "image_filenames = os.walk(datasetDir+'/test')\n",
    "for(root,dirs,images) in image_filenames:\n",
    "    for image_name in images:\n",
    "        img = Image.open(datasetDir+'/test/'+image_name).resize((h,w)).convert('RGB')\n",
    "        img_array = np.array(img)\n",
    "        img_array = np.expand_dims(img_array, 0)\n",
    "        img_array = tf.cast(img_array,tf.float32)/255\n",
    "        res = model.predict(x = img_array)\n",
    "        prediction = np.argmax(res)\n",
    "        results[image_name] = prediction\n",
    "print('Work done.')\n",
    "print('Writing results...')\n",
    "create_csv(results,csv_dir)\n",
    "print('Results Written.')\n",
    "\n",
    "ValidationCreation.delete_validation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Face2Text)",
   "language": "python",
   "name": "pycharm-548e7b6f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
