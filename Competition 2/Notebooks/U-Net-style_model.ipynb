{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net style model: main notebook\n",
    "In this notebook we are going to define the main structure of the model and we are going to train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing needed packages and definition of useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "seed = 1234\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "datasetDir = os.path.join(cwd, 'Segmentation_Dataset')\n",
    "trainingImgDir = os.path.join(datasetDir, 'training', 'images')\n",
    "trainingMaskDir = os.path.join(datasetDir, 'training', 'masks')\n",
    "validImgDir = os.path.join(datasetDir, 'validation', 'images')\n",
    "validMaskDir = os.path.join(datasetDir, 'validation', 'masks')\n",
    "\n",
    "bs = 40\n",
    "\n",
    "img_h = 256\n",
    "img_w = 256\n",
    "\n",
    "now = datetime.now().strftime('%b%d_%H-%M-%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the training generator and the validation generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set ( Images and Masks generators )\n",
    "trainImgDataGen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=2,\n",
    "    height_shift_range=2,\n",
    "    zoom_range=0.05,\n",
    "    #horizontal_flip=False,\n",
    "    #vertical_flip=False,\n",
    "    fill_mode='reflect',\n",
    "    #cval=0,\n",
    "    rescale=1. / 255\n",
    ")\n",
    "\n",
    "trainMaskDataGen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=2,\n",
    "    height_shift_range=2,\n",
    "    zoom_range=0.05,\n",
    "    #horizontal_flip=False,\n",
    "    #vertical_flip=False,\n",
    "    fill_mode='reflect',\n",
    "    #cval=0,\n",
    "    rescale=1. / 255,\n",
    "    dtype=tf.int32\n",
    ")\n",
    "\n",
    "trainImgGen = trainImgDataGen.flow_from_directory(trainingImgDir,\n",
    "                                                  class_mode=None,\n",
    "                                                  color_mode='rgb',\n",
    "                                                  shuffle=True,\n",
    "                                                  seed=seed,\n",
    "                                                  batch_size=bs\n",
    "                                                  )\n",
    "\n",
    "trainMaskGen = trainMaskDataGen.flow_from_directory(trainingMaskDir,\n",
    "                                                    class_mode=None,\n",
    "                                                    color_mode='grayscale',\n",
    "                                                    shuffle=True,\n",
    "                                                    seed=seed,\n",
    "                                                    batch_size=bs \n",
    "                                                    )\n",
    "\n",
    "trainGen = (pair for pair in zip(trainImgGen, trainMaskGen))\n",
    "\n",
    "# Validation set ( Images and Masks generators )\n",
    "validImgDataGen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1. / 255\n",
    ")\n",
    "\n",
    "validMaskDataGen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    dtype=tf.int32\n",
    ")\n",
    "\n",
    "validImgGen = validImgDataGen.flow_from_directory(validImgDir,\n",
    "                                                  class_mode=None,\n",
    "                                                  color_mode='rgb',\n",
    "                                                  shuffle=False,\n",
    "                                                  batch_size=bs\n",
    "                                                  )\n",
    "\n",
    "validMaskGen = validMaskDataGen.flow_from_directory(validMaskDir,\n",
    "                                                    class_mode=None,\n",
    "                                                    color_mode='grayscale',\n",
    "                                                    shuffle=False,\n",
    "                                                    batch_size=bs\n",
    "                                                    )\n",
    "\n",
    "validGen = (pair for pair in zip(validImgGen, validMaskGen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Structure: U-Net like structure, so we use skipping connections in order to have a better predictions result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inp1 = tf.keras.Input(shape=(img_h,img_w,3)) \n",
    "\n",
    "# Encoding Part\n",
    "\n",
    "c1 = tf.keras.layers.Conv2D(filters=16,kernel_size=(3,3),padding='same',activation=tf.keras.activations.relu)(inp1)\n",
    "c1 = tf.keras.layers.Conv2D(filters=16,kernel_size=(3,3),padding='same',activation=tf.keras.activations.relu)(c1)\n",
    "m = tf.keras.layers.MaxPool2D(pool_size=(2,2))(c1)\n",
    "#128\n",
    "c2= tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3),padding='same',activation=tf.keras.activations.relu)(m)\n",
    "c2= tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3),padding='same',activation=tf.keras.activations.relu)(c2)\n",
    "m = tf.keras.layers.MaxPool2D(pool_size=(2,2))(c2)\n",
    "#64\n",
    "c3 = tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3),padding='same',activation=tf.keras.activations.relu)(m)\n",
    "c3 = tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3),padding='same',activation=tf.keras.activations.relu)(c3)\n",
    "m = tf.keras.layers.MaxPool2D(pool_size=(2,2))(c3)\n",
    "#32\n",
    "c4 = tf.keras.layers.Conv2D(filters=128,kernel_size=(3,3),padding='same',activation=tf.keras.activations.relu)(m)\n",
    "c4 = tf.keras.layers.Conv2D(filters=128,kernel_size=(3,3),padding='same',activation=tf.keras.activations.relu)(c4)\n",
    "m = tf.keras.layers.MaxPool2D(pool_size=(2,2))(c4)\n",
    "#16\n",
    "c5 = tf.keras.layers.Conv2D(filters=256,kernel_size=(3,3),padding='same',activation=tf.keras.activations.relu)(m)\n",
    "c5 = tf.keras.layers.Conv2D(filters=256,kernel_size=(3,3),padding='same',activation=tf.keras.activations.relu)(c5)\n",
    "m = tf.keras.layers.MaxPool2D(pool_size=(2,2))(c5)\n",
    "#8\n",
    "\n",
    "c6 = tf.keras.layers.Conv2D(filters=512,kernel_size=(3,3),padding='same',activation=tf.keras.activations.relu)(m)\n",
    "c6 = tf.keras.layers.Conv2D(filters=512,kernel_size=(3,3),padding='same',activation=tf.keras.activations.relu)(c6)\n",
    "\n",
    "#Decoding path with skipping connections\n",
    "\n",
    "ct = tf.keras.layers.Conv2DTranspose(filters=256, kernel_size=(3,3), padding='same', strides=(2,2), activation=tf.keras.activations.relu)(c6)\n",
    "conc = tf.keras.layers.Concatenate()([ct,c5])\n",
    "c = tf.keras.layers.Conv2D(filters = 256, kernel_size=(3,3),activation = tf.keras.activations.relu,padding='same')(conc)\n",
    "c = tf.keras.layers.Conv2D(filters = 256, kernel_size=(3,3),activation = tf.keras.activations.relu,padding='same')(c)\n",
    "\n",
    "ct = tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=(3,3), padding='same', strides=(2,2), activation=tf.keras.activations.relu)(c)\n",
    "conc = tf.keras.layers.Concatenate()([ct,c4])\n",
    "c = tf.keras.layers.Conv2D(filters = 128, kernel_size=(3,3),activation = tf.keras.activations.relu,padding='same')(conc)\n",
    "c = tf.keras.layers.Conv2D(filters = 128, kernel_size=(3,3),activation = tf.keras.activations.relu,padding='same')(c)\n",
    "\n",
    "ct = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=(3,3),padding='same',strides=(2,2),activation=tf.keras.activations.relu)(c)\n",
    "conc = tf.keras.layers.Concatenate()([ct,c3])\n",
    "c = tf.keras.layers.Conv2D(filters = 64, kernel_size=(3,3),activation = tf.keras.activations.relu,padding='same')(conc)\n",
    "c = tf.keras.layers.Conv2D(filters = 64, kernel_size=(3,3),activation = tf.keras.activations.relu,padding='same')(c)\n",
    "\n",
    "ct = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=(3,3),padding='same',strides=(2,2),activation=tf.keras.activations.relu)(c)\n",
    "conc = tf.keras.layers.Concatenate()([ct,c2])\n",
    "c = tf.keras.layers.Conv2D(filters = 32, kernel_size=(3,3),activation = tf.keras.activations.relu,padding='same')(conc)\n",
    "c = tf.keras.layers.Conv2D(filters = 32, kernel_size=(3,3),activation = tf.keras.activations.relu,padding='same')(c)\n",
    "\n",
    "ct = tf.keras.layers.Conv2DTranspose(filters=16, kernel_size=(3,3),padding='same',strides=(2,2),activation=tf.keras.activations.relu)(c)\n",
    "conc = tf.keras.layers.Concatenate()([ct,c1])\n",
    "c = tf.keras.layers.Conv2D(filters = 16, kernel_size=(3,3) , activation = tf.keras.activations.relu,padding='same')(conc)\n",
    "c = tf.keras.layers.Conv2D(filters = 8, kernel_size=(1,1) , activation = tf.keras.activations.relu,padding='same')(c)\n",
    "\n",
    "#Prediction Layer\n",
    "#Two feature maps in output, distinguishing between background and building\n",
    "out = tf.keras.layers.Conv2D(filters=2,kernel_size=(1,1),padding='same',activation=tf.keras.activations.softmax)(c)\n",
    "\n",
    "#Creation of the model\n",
    "model = tf.keras.Model(inp1,out)\n",
    "\n",
    "# output 256x256x1\n",
    "model.summary()\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the metric taken into consideration: Intersection over Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_IoU(y_true, y_pred):\n",
    "    # from pobability to predicted class {0, 1}\n",
    "    y_pred = tf.argmax(input=y_pred, axis=3)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_pred = tf.expand_dims(y_pred, -1)\n",
    "    # A and B\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    # A or B\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "    # IoU\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDir = os.path.join(cwd, 'models')\n",
    "if not os.path.exists(modelDir):\n",
    "    os.makedirs(modelDir)\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=[my_IoU])\n",
    "\n",
    "#Fit_generator in order to reduce the overhead due to mantaining the data in memory\n",
    "model.fit_generator(trainGen,\n",
    "                    epochs=50, verbose= 2,\n",
    "                   steps_per_epoch=len(trainImgGen),\n",
    "                   validation_data=validGen,\n",
    "                   validation_steps=len(validImgGen)\n",
    ")\n",
    "\n",
    "model.save(os.path.join(modelDir, 'model_' + now + '.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining useful functions and performing predictions over the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvDir = os.path.join(cwd, 'csv_files')\n",
    "\n",
    "if not os.path.exists(csvDir):\n",
    "    os.makedirs(csvDir)\n",
    "\n",
    "def create_csv(results, results_dir):\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir,csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('ImageId,EncodedPixels,Width,Height\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(key + ',' + str(value) + ',' + '256' + ',' + '256' + '\\n')\n",
    "\n",
    "def rle_encode(img):\n",
    "    # Flatten column-wise\n",
    "    pixels = img.numpy().T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "results = {}\n",
    "print('Test set predictions...')\n",
    "image_filenames = os.listdir(os.path.join(datasetDir, 'test', 'images', 'img'))\n",
    "for image in image_filenames:\n",
    "        img = Image.open(os.path.join(datasetDir, 'test', 'images', 'img', image))\n",
    "        img_array = np.array(img)\n",
    "        img_array = np.expand_dims(img_array, 0)\n",
    "        img_array = tf.cast(img_array,tf.float32)/255\n",
    "        prediction = model.predict(x = img_array)\n",
    "        image = image[0:image.find('.')]\n",
    "        results[image] = rle_encode(tf.argmax(input=prediction, axis=3))\n",
    "\n",
    "print('Work done.')\n",
    "print('Writing results...')\n",
    "create_csv(results, csvDir)\n",
    "print('Results Written.')\n",
    "\n",
    "\n",
    "f, axarr = plt.subplots(1,2)\n",
    "\n",
    "for image in image_filenames:\n",
    "        img = Image.open(os.path.join(datasetDir, 'test', 'images', 'img', image))\n",
    "        img_array = np.array(img)\n",
    "        img_array = np.expand_dims(img_array, 0)\n",
    "        img_array = tf.cast(img_array,tf.float32)/255\n",
    "        prediction = model.predict(x = img_array)\n",
    "        img_array = np.array(img_array)\n",
    "        prediction = np.array(tf.argmax(prediction,-1))\n",
    "        axarr[0].imshow(img_array.squeeze())\n",
    "        axarr[1].imshow(prediction.squeeze())\n",
    "        f.show()\n",
    "        time.sleep(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Face2Text)",
   "language": "python",
   "name": "pycharm-548e7b6f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
