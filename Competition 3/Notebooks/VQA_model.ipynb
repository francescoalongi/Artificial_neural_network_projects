{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Question Answering model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing phase and creation of local variables needed for subsequent cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import random\n",
    "from matplotlib.image import imread\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "classes = {'0': 0,\n",
    "'1': 1,\n",
    "'10': 2,\n",
    "'2': 3,\n",
    "'3': 4,\n",
    "'4': 5,\n",
    "'5': 6,\n",
    "'6': 7,\n",
    "'7': 8,\n",
    "'8': 9,\n",
    "'9': 10,\n",
    "'no': 11,\n",
    "'yes': 12}\n",
    "\n",
    "bst= 150\n",
    "bsv= 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of **CustomDataGenerator** class: in this case this generator reads the question and take the features of the correspondent image. It outputs the tokenized question and the features vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, data, batch_size, tokenizer, featuresMap, seed=1234, num_classes=13, shuffle=True, test = False):\n",
    "        self.data = data  # data on wich perform\n",
    "        self.batch_size = batch_size  # batch size\n",
    "        self.featuresMap = featuresMap  # features of the images obtained from a pretrained model\n",
    "        self.seed = seed  # seed for the shuffle operations\n",
    "        self.num_classes = num_classes  # number of classes (13 in our case)\n",
    "        self.test = test\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()  # boolean to say if to perform shuffle on each batch or not\n",
    "        self.tok = tokenizer\n",
    "        # set the seed\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        'method for the lenght of the generator'\n",
    "        return int(np.floor(len(self.data) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'returns a batch of (image, question) and answer'\n",
    "        indexes = self.indexes[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        data_temp = [self.data[k] for k in indexes]\n",
    "        X = self._generate_X(data_temp)\n",
    "        if self.test == False:\n",
    "            y = self._generate_y(data_temp)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def _generate_X(self, data_temp):\n",
    "        'generates the batch of (image,question)'\n",
    "        img_array = np.empty((self.batch_size, 512))\n",
    "        question_array = np.empty((self.batch_size, 41))\n",
    "        for i, dictionary in enumerate(data_temp):\n",
    "            filename = dictionary.get('image_filename')\n",
    "            image = np.array(self.featuresMap[filename])\n",
    "            img_array[i,] = image.squeeze()\n",
    "            token = self.tok.texts_to_sequences([dictionary.get('question')])\n",
    "            padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(token, maxlen=41)\n",
    "            padded_sequence = padded_sequence.squeeze()\n",
    "            question_array[i,] = padded_sequence\n",
    "        x1 = np.array(img_array)\n",
    "        x2 = np.array(question_array)\n",
    "        return [x1, x2]\n",
    "\n",
    "    def _generate_y(self, data_temp):\n",
    "        'generates the one hot encoding of the answer'\n",
    "        answer_array = []\n",
    "        for dictionary in data_temp:\n",
    "            answer_array.append(\n",
    "                tf.keras.utils.to_categorical(classes[dictionary.get('answer')], num_classes=self.num_classes))\n",
    "        y = np.array(answer_array)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading of the necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "datasetName = '../input/ann-and-dl-vqa/dataset_vqa'\n",
    "jsonFiles = '../input/json-files'\n",
    "tensors = '../input/vgg19-tensors-gap'\n",
    "trainJsonName = 'train_data.json'\n",
    "validJsonName = 'valid_data.json'\n",
    "\n",
    "imagesPath = os.path.join(datasetName, 'train')\n",
    "trainJsonPath = os.path.join(jsonFiles, trainJsonName)\n",
    "validJsonPath = os.path.join(jsonFiles, validJsonName)\n",
    "\n",
    "trainTensors = os.path.join(tensors, \"train_tensors_VGG19_GAP.json\")\n",
    "validTensors = os.path.join(tensors, \"valid_tensors_VGG19_GAP.json\")\n",
    "\n",
    "seed=1234\n",
    "\n",
    "with open(trainJsonPath,'r') as json_file_train, open (validJsonPath, 'r') as json_file_valid:\n",
    "    data_train = json.load(json_file_train).get('questions')\n",
    "    data_valid = json.load(json_file_valid).get('questions')\n",
    "    json_file_train.close()\n",
    "    json_file_valid.close()\n",
    "\n",
    "train_size = len(data_train)\n",
    "valid_size = len(data_valid)\n",
    "\n",
    "with open(trainTensors,'r') as json_file_train, open (validTensors, 'r') as json_file_valid:\n",
    "    train_features = json.load(json_file_train)\n",
    "    valid_features = json.load(json_file_valid)\n",
    "    json_file_train.close()\n",
    "    json_file_valid.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the Tokenizer and instantiation of two CustomDataGenerator, one for training and the other for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [el['question'] for el in data_train]\n",
    "words = set()\n",
    "maxLength = 0\n",
    "for q in questions:\n",
    "    seq = tf.keras.preprocessing.text.text_to_word_sequence(q)\n",
    "    if maxLength < len(seq): maxLength = len(seq)\n",
    "    for x in seq:\n",
    "        words.add(x)\n",
    "# number of different words in our sequences or vocaboulary size\n",
    "n_words = len(words)\n",
    "# Tokenizer and indexes creation\n",
    "tok = tf.keras.preprocessing.text.Tokenizer(num_words=n_words)\n",
    "tok.fit_on_texts(questions)\n",
    "\n",
    "gen_train = CustomDataGenerator(data_train,bst, tok,train_features,seed,num_classes = 13,shuffle = True,test = False)\n",
    "gen_val = CustomDataGenerator(data_valid,bsv, tok,valid_features,seed,num_classes = 13, shuffle = False, test = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the structure of the Neural Network and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Structure of the CNN and of the RNN\n",
    "\n",
    "#Convolutional Neural Network\n",
    "\n",
    "inp1 = tf.keras.Input(shape = (512))\n",
    "\n",
    "dense1 = tf.keras.layers.Dense(units=256, activation= tf.keras.activations.relu, kernel_initializer = 'he_uniform')(inp1)\n",
    "\n",
    "#Recurrent Neural Network with LSTM\n",
    "inp2 = tf.keras.Input(name='input_LSTM', shape=(41))\n",
    "r = tf.keras.layers.Embedding(input_dim=71, output_dim=32)(inp2)\n",
    "r = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=256, return_sequences=True))(r)\n",
    "r = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=256, return_sequences=False))(r)\n",
    "dense2 = tf.keras.layers.Dense(units=256, activation = tf.keras.activations.relu, kernel_initializer = 'he_uniform')(r)\n",
    "\n",
    "conc = tf.keras.layers.Concatenate()([dense1, dense2])\n",
    "d = tf.keras.layers.Dense(units=1024, activation=tf.keras.activations.relu, kernel_initializer = 'he_uniform')(conc)\n",
    "d = tf.keras.layers.Dropout(0.3)(d)\n",
    "d = tf.keras.layers.Dense(units=1024, activation=tf.keras.activations.relu, kernel_initializer = 'he_uniform')(d)\n",
    "d = tf.keras.layers.Dropout(0.3)(d)\n",
    "out = tf.keras.layers.Dense(units=13, activation=tf.keras.activations.softmax)(d)\n",
    "\n",
    "model = tf.keras.Model([inp1, inp2], out)\n",
    "model.summary()\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n",
    "metrics = ['accuracy']\n",
    "\n",
    "model.compile(metrics=metrics, optimizer=optimizer, loss=loss)\n",
    "\n",
    "model.fit_generator(gen_train, steps_per_epoch=len(gen_train), validation_data=gen_val, validation_steps=len(gen_val), epochs = 10 , workers = 4)#,callbacks = [callback_chkpt])\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "modelName = 'model_' + str(datetime.now().strftime('%b%d_%H-%M-%S')) + '.h5'\n",
    "model.save(modelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing phase of the Neural Network and creation of the .csv file for submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(results, results_dir='./'):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(str(key) + ',' + str(value) + '\\n')\n",
    "\n",
    "testJsonName = 'test_data.json'\n",
    "\n",
    "testJsonPath = os.path.join(datasetName, testJsonName)\n",
    "\n",
    "testTensors = os.path.join(tensors, \"test_tensors_VGG19_GAP.json\")\n",
    "seed=1234\n",
    "\n",
    "with open(testJsonPath,'r') as json_file_test:\n",
    "    data_test = json.load(json_file_test).get('questions')\n",
    "    json_file_test.close()\n",
    "\n",
    "with open(testTensors,'r') as json_file_test:\n",
    "    test_features = json.load(json_file_test)\n",
    "    json_file_test.close()\n",
    "\n",
    "print('Test set length:' + str(len(data_test)))\n",
    "test_gen = CustomDataGenerator(data_test,1,tok,test_features, seed = seed, shuffle=False,test = True)\n",
    "\n",
    "predictions = model.predict_generator(test_gen)\n",
    "print('Predictions vector length:' + str(len(predictions)))\n",
    "\n",
    "results = {}\n",
    "\n",
    "work_pr = []\n",
    "for i in range(len(predictions)):\n",
    "    work_pr.append(tf.argmax(predictions[i], axis=-1).numpy())\n",
    "\n",
    "\n",
    "for i in range(len(data_test)):\n",
    "    results[data_test[i].get('question_id')] = work_pr[i]\n",
    "\n",
    "create_csv(results)\n",
    "print('CSV written!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [profile===thesis]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
